# analisis/notebooks/3.2.1_modelo_aumentado_v1.ipynb

## Procedimiento paso a paso

1. Carga y preparación idéntica a v0 (enriched + renombres + imputaciones por barrio).
2. Selección de variables reducidas (`features_reducidas`):
   - Originales clave: `area`, `parqueaderos`, `administracion`, `banos`, `antiguedad`, `habitaciones`, `estado`, `gimnasio`, `ascensor`, `piscina`, `zona_de_bbq`, `latitud`, `longitud`.
   - `barrio_top`: categórica comprimida en 4 barrios relevantes vs `OTROS`.
3. Split train/holdout (20%).
4. Pipeline de preprocesamiento:
   - `area`: imputación mediana + `log` (FunctionTransformer).
   - Resto numéricas: `SimpleImputer(median)`.
   - Categóricas: `SimpleImputer(most_frequent)` + `OneHotEncoder`.
5. Modelo: `XGBRegressor` (100 árboles, profundidad 6, lr=0.1).
6. Entrenamiento sobre `y = log(precio_venta)`.
7. Evaluación en hold-out (sin CV): reporta métricas en escala original (deslog-transformadas):
   - RMSE≈254.7M, MAE≈136.5M, R²≈0.914.
8. Exportación del pipeline y metadatos a `../data/models/xgboost_model_2.1.pkl`.
