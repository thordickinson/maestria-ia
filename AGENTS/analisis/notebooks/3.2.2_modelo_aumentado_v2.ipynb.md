# analisis/notebooks/3.2.2_modelo_aumentado_v2.ipynb

## Procedimiento paso a paso

1. Misma preparación que v1 (enriched + renombres + imputaciones + features reducidas + `barrio_top`).
2. Búsqueda aleatoria de hiperparámetros para `XGBRegressor` con `RandomizedSearchCV` (CV=3) optimizando `RMSE`:
   - Espacio: `n_estimators`, `max_depth`, `learning_rate`, `subsample`, `colsample_bytree`, `reg_alpha`, `reg_lambda`.
   - Mejores parámetros (ejemplo): `n_estimators=500`, `max_depth=9`, `learning_rate=0.05`, `subsample=0.8`, `colsample_bytree=0.8`, `reg_alpha=0`, `reg_lambda=1`.
3. Evaluación en hold-out con el mejor estimador:
   - RMSE≈233.5M, MAE≈122.0M, R²≈0.928 (mejora vs v1).
4. Exportación del pipeline y metadatos a `../data/models/xgboost_model_2.2.pkl`.
5. Conclusiones: el tuning mejora todas las métricas (~8–9% en RMSE vs v1) con un modelo aún compacto.
